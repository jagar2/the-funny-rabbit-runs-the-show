{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“– ðŸŽ¶ Understanding Quantization with the THX Deep Note ðŸŽ¶\n",
    "\n",
    "![](./assets/figures/THX-details.jpg)\n",
    "\n",
    "![](./assets/figures/thx-logo.webp)\n",
    "\n",
    "Youâ€™ve probably heard the iconic THX Deep Note in movie theatersâ€”a deep, spine-tingling crescendo that fills the room with cinematic magic. But what makes it sound so immersive? Let's break it down with some quantization magic!\n",
    "\n",
    "## ðŸ§© What is Quantization?\n",
    "\n",
    "Quantization in audio and digital signals is like snapping the smooth curves of sound into tiny steps that computers can handle. Think of it as:\n",
    "\n",
    "- Frequency Quantization: Slicing up the range of pitches (frequencies) into specific levels.\n",
    "- Amplitude Quantization: Breaking the loudness (volume) into small, fixed units.\n",
    "\n",
    "This process is essential because computers need numbers, not continuous waves, to process and reproduce sounds.\n",
    "\n",
    "## ðŸ”§ Hardware Quantization and Oscillators\n",
    "\n",
    "When the THX Deep Note was first created in 1983, it was generated by analog-to-digital oscillators on a computer. These oscillators were programmed to produce a random mix of frequencies that climbed or descended to form that rich, harmonious crescendo.\n",
    "\n",
    "Here's where it gets fun:\n",
    "\n",
    "- Oscillators are like digital musicians, each producing a pure tone (like a single note on a keyboard).\n",
    "- In the Deep Note, 20 oscillators start at random frequencies within a range and slowly move toward their final frequencies.\n",
    "- The random starting points give the sound its mysterious, evolving quality.\n",
    "\n",
    "But thereâ€™s a catch: the synthesizer couldnâ€™t handle infinite smooth transitions! The frequencies were quantized, meaning each oscillator could only hit certain predefined steps, leading to a staircase-like journey instead of a perfect glide.\n",
    "\n",
    "## ðŸŽ¼ The Randomness Behind the Magic\n",
    "\n",
    "The THX Deep Note is never _exactly_ the same! Each performance of the original algorithm introduces small random variations in:\n",
    "\n",
    "- Initial frequencies: The oscillators start at different random pitches, which makes the beginning sound slightly chaotic.\n",
    "- Timing: The speed at which each oscillator glides to its final frequency can vary.\n",
    "\n",
    "This randomness gives the Deep Note its unique, almost \"alive\" quality. Even though itâ€™s a digitally generated sound, it feels dynamic and organicâ€”thanks to the clever use of randomness and quantization.\n",
    "\n",
    "## ðŸ§  How Frequency Quantization Shapes the Deep Note\n",
    "\n",
    "Quantizing the frequencies means:\n",
    "\n",
    "1. The range of pitches was split into discrete steps that the synthesizer hardware could handle.\n",
    "2. As oscillators climbed or descended, they â€œsnappedâ€ to the closest quantized step, adding a subtle, mechanical texture to the sound.\n",
    "\n",
    "Think of it like climbing a staircase instead of sliding up a ramp. The oscillators couldnâ€™t hit every micro-pitch, but they got close enough to create the smooth illusion we hear.\n",
    "\n",
    "## ðŸŽ‰ Fun Facts About the Deep Note\n",
    "\n",
    "- Dynamic Chaos: The Deep Note starts as pure chaos (random pitches) and resolves into a powerful harmony. This transition mirrors the tension and release we love in music!\n",
    "- Impossible to Play Live: The original Deep Note algorithm uses so many oscillators and random variations that itâ€™s essentially unplayable by human hands. Computers = ultimate rock stars!\n",
    "- Revised for Modern Tech: In 2015, a new version of the Deep Note was created with modern tools, featuring up to 80 oscillators and higher precision, but the essence of randomness and quantization remained.\n",
    "- Not Just Audio: The Deep Note isnâ€™t just soundâ€”itâ€™s an experience. The use of quantization ensures the sound feels consistent across theaters, no matter the size of the speakers.\n",
    "\n",
    "## ðŸ’¡ Why Does It Matter?\n",
    "\n",
    "Quantization is the backbone of how we digitize sound. Without it:\n",
    "\n",
    "- Weâ€™d still be stuck with analog recordings, unable to share crystal-clear sounds online.\n",
    "- Sounds like the THX Deep Note wouldnâ€™t exist in their iconic form.\n",
    "\n",
    "So next time you hear the THX Deep Note, remember: itâ€™s not just cool audio engineeringâ€”itâ€™s oscillators, randomness, and quantization working together to blow your mind!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "from IPython.display import Audio, display\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def quantize_in_frequency_domain(samples: np.ndarray, n_bits: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Quantize an audio signal in the frequency domain.\n",
    "\n",
    "    Parameters:\n",
    "        samples (np.ndarray): The input audio signal (int16 format).\n",
    "        n_bits (int): Number of bits to use for quantization (4 â‰¤ n_bits â‰¤ 16).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The quantized audio signal (int16 format).\n",
    "    \"\"\"\n",
    "    # Convert samples to float in range [-1.0, 1.0]\n",
    "    samples_float = samples.astype(np.float32) / 32768.0\n",
    "\n",
    "    # Perform FFT to move to the frequency domain\n",
    "    fft_coefficients = np.fft.fft(samples_float)\n",
    "\n",
    "    # Quantization in the frequency domain\n",
    "    # Scale coefficients for quantization\n",
    "    magnitude = np.abs(fft_coefficients)\n",
    "    phase = np.angle(fft_coefficients)\n",
    "\n",
    "    # Number of quantization levels\n",
    "    levels = 2**n_bits\n",
    "\n",
    "    # Quantize magnitudes\n",
    "    quantized_magnitude = np.round(magnitude * (levels / np.max(magnitude))) / (\n",
    "        levels / np.max(magnitude)\n",
    "    )\n",
    "\n",
    "    # Reconstruct quantized coefficients\n",
    "    quantized_coefficients = quantized_magnitude * np.exp(1j * phase)\n",
    "\n",
    "    # Perform IFFT to return to the time domain\n",
    "    quantized_signal = np.fft.ifft(quantized_coefficients).real\n",
    "\n",
    "    # Convert back to int16\n",
    "    quantized_int16 = (quantized_signal * 32768.0).astype(np.int16)\n",
    "\n",
    "    return quantized_int16\n",
    "\n",
    "\n",
    "for i in [2, 4, 6, 8, 10, 12, 16]:\n",
    "    # Load MP3\n",
    "    input_file = \"./assets/sounds/THX_Deep_Note.mp3\"\n",
    "    output_wav = f\"./assets/sounds/output_frequency_quantized_at_{i}_bits.mp3\"\n",
    "    target_bit_depth = (\n",
    "        i  # Change this between 4 and 16 for different quantization depths.\n",
    "    )\n",
    "\n",
    "    # Load MP3 file with pydub\n",
    "    audio = AudioSegment.from_mp3(input_file)\n",
    "\n",
    "    # Extract raw samples as a NumPy array\n",
    "    channels = audio.channels\n",
    "    frame_rate = audio.frame_rate\n",
    "    samples = np.array(audio.get_array_of_samples(), dtype=np.int16)\n",
    "\n",
    "    # If there are multiple channels, split and process each channel separately\n",
    "    if channels > 1:\n",
    "        samples = samples.reshape(-1, channels)\n",
    "        quantized_samples = np.zeros_like(samples, dtype=np.int16)\n",
    "        for ch in range(channels):\n",
    "            quantized_samples[:, ch] = quantize_in_frequency_domain(\n",
    "                samples[:, ch], target_bit_depth\n",
    "            )\n",
    "    else:\n",
    "        quantized_samples = quantize_in_frequency_domain(samples, target_bit_depth)\n",
    "\n",
    "    # Convert back to AudioSegment for saving as WAV\n",
    "    quantized_audio = AudioSegment(\n",
    "        quantized_samples.tobytes(),\n",
    "        frame_rate=frame_rate,\n",
    "        sample_width=2,  # 16 bits = 2 bytes\n",
    "        channels=channels,\n",
    "    )\n",
    "\n",
    "    # Export as WAV\n",
    "    quantized_audio.export(output_wav, format=\"mp3\")\n",
    "\n",
    "    print(f\"Saved frequency-quantized audio as {output_wav}\")\n",
    "\n",
    "# List of audio files and their labels\n",
    "audio_files = [\n",
    "    \"./assets/sounds/output_frequency_quantized_at_2_bits.mp3\",\n",
    "    \"./assets/sounds/output_frequency_quantized_at_4_bits.mp3\",\n",
    "    \"./assets/sounds/output_frequency_quantized_at_6_bits.mp3\",\n",
    "    \"./assets/sounds/output_frequency_quantized_at_8_bits.mp3\",\n",
    "    \"./assets/sounds/output_frequency_quantized_at_10_bits.mp3\",\n",
    "    \"./assets/sounds/output_frequency_quantized_at_12_bits.mp3\",\n",
    "    \"./assets/sounds/output_frequency_quantized_at_16_bits.mp3\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to convert MP3 to WAV\n",
    "def convert_mp3_to_wav(mp3_path):\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path\n",
    "\n",
    "\n",
    "# Function to plot waveform, FFT, and embed audio\n",
    "def plot_waveform_and_fft(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    original_file_path = file_path.copy()\n",
    "\n",
    "    # Convert MP3 to WAV if necessary\n",
    "    if file_path.endswith(\".mp3\"):\n",
    "        file_path = convert_mp3_to_wav(file_path)\n",
    "\n",
    "    # Read audio file\n",
    "    rate, data = read(file_path)\n",
    "\n",
    "    # Convert stereo to mono if necessary\n",
    "    if len(data.shape) > 1:\n",
    "        data = np.mean(data, axis=1)\n",
    "\n",
    "    # Generate waveform plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(data, color=\"blue\")\n",
    "    plt.title(f\"Waveform: {os.path.basename(file_path)}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Compute FFT\n",
    "    N = len(data)\n",
    "    fft_data = np.fft.fft(data)\n",
    "    fft_freqs = np.fft.fftfreq(N, 1 / rate)\n",
    "\n",
    "    # Plot FFT (magnitude spectrum)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(fft_freqs[: N // 2], np.abs(fft_data)[: N // 2], color=\"red\")\n",
    "    plt.title(f\"FFT Magnitude Spectrum: {os.path.basename(file_path)}\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Embed audio\n",
    "    display(Audio(original_file_path))\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import panel as pn\n",
    "from scipy.io.wavfile import read\n",
    "from IPython.display import Audio\n",
    "from pydub import AudioSegment\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "\n",
    "# Function to convert MP3 to WAV\n",
    "def convert_mp3_to_wav(mp3_path):\n",
    "    audio = AudioSegment.from_mp3(mp3_path)\n",
    "    wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "    return wav_path\n",
    "\n",
    "\n",
    "# Function to plot waveform, FFT, and embed audio\n",
    "def plot_waveform_and_fft(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return f\"File not found: {file_path}\"\n",
    "\n",
    "    # Convert MP3 to WAV if necessary\n",
    "    if file_path.endswith(\".mp3\"):\n",
    "        file_path = convert_mp3_to_wav(file_path)\n",
    "\n",
    "    # Read audio file\n",
    "    rate, data = read(file_path)\n",
    "\n",
    "    # Convert stereo to mono if necessary\n",
    "    if len(data.shape) > 1:\n",
    "        data = np.mean(data, axis=1)\n",
    "\n",
    "    # Generate waveform plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(data, color=\"blue\")\n",
    "    plt.title(f\"Waveform: {os.path.basename(file_path)}\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    waveform_plot = pn.pane.Matplotlib(plt.gcf(), dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    # Compute FFT\n",
    "    N = len(data)\n",
    "    fft_data = np.fft.fft(data)\n",
    "    fft_freqs = np.fft.fftfreq(N, 1 / rate)\n",
    "\n",
    "    # Plot FFT (magnitude spectrum)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(fft_freqs[: N // 2], np.abs(fft_data)[: N // 2], color=\"red\")\n",
    "    plt.title(f\"FFT Magnitude Spectrum: {os.path.basename(file_path)}\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    fft_plot = pn.pane.Matplotlib(plt.gcf(), dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "    # Embed audio\n",
    "    audio_player = pn.pane.Audio(\n",
    "        file_path, name=f\"Audio: {os.path.basename(file_path)}\", autoplay=False\n",
    "    )\n",
    "\n",
    "    return pn.Column(waveform_plot, fft_plot, audio_player)\n",
    "\n",
    "\n",
    "# List of audio files and their labels\n",
    "audio_files = {\n",
    "    \"2 Bits\": \"./assets/sounds/output_frequency_quantized_at_2_bits.mp3\",\n",
    "    \"4 Bits\": \"./assets/sounds/output_frequency_quantized_at_4_bits.mp3\",\n",
    "    \"6 Bits\": \"./assets/sounds/output_frequency_quantized_at_6_bits.mp3\",\n",
    "    \"8 Bits\": \"./assets/sounds/output_frequency_quantized_at_8_bits.mp3\",\n",
    "    \"10 Bits\": \"./assets/sounds/output_frequency_quantized_at_10_bits.mp3\",\n",
    "    \"12 Bits\": \"./assets/sounds/output_frequency_quantized_at_12_bits.mp3\",\n",
    "    \"16 Bits\": \"./assets/sounds/output_frequency_quantized_at_16_bits.mp3\",\n",
    "}\n",
    "\n",
    "# Create tabs for each bit depth\n",
    "tabs = pn.Tabs()\n",
    "for bit_depth, file_path in audio_files.items():\n",
    "    tabs.append((bit_depth, plot_waveform_and_fft(file_path)))\n",
    "\n",
    "# Serve the panel\n",
    "tabs.servable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engr131_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
