{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ASSIGNMENT CONFIG\n",
    "requirements: requirements.txt\n",
    "solutions_pdf: false\n",
    "export_cell: false\n",
    "generate: \n",
    "    pdf: flase\n",
    "    filtering: false\n",
    "    pagebreaks: false\n",
    "    zips: false\n",
    "files: [.responses.json]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⌛️ Quiz 9 Practice - Classes\n",
    "\n",
    "This quiz will evaluate your mastery of using classes in Python. Functions provide a way to isolate code that you want to use repeatedly, and they allow you to pass in data to the code and get data back out of the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entering Your Information for Credit\n",
    "\n",
    "To receive credit for assignments it is important we can identify your work from others. To do this we will ask you to enter your information in the following code block.\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "Run the block of code at the top of the notebook that imports and sets up the autograder. This will allow you to check your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from subprocess import call\n",
    "import sys\n",
    "\n",
    "package_name = 'ENGR131_Util_2024'\n",
    "version = '0.1.11'\n",
    "package_version = f'{package_name}=={version}'\n",
    "\n",
    "try:\n",
    "    # Check if the package and version are installed\n",
    "    pkg_resources.require(package_version)\n",
    "    print(f'{package_version} is already installed.')\n",
    "except pkg_resources.DistributionNotFound:\n",
    "    # If not installed, install the package\n",
    "    print(f'{package_version} not found. Installing...')\n",
    "    call([sys.executable, '-m', 'pip', 'install', package_version])\n",
    "except pkg_resources.VersionConflict:\n",
    "    # If a different version is installed, you can choose to upgrade/downgrade\n",
    "    installed_packages = {dist.key: dist.version for dist in pkg_resources.working_set}\n",
    "    installed_version = installed_packages.get(package_name.lower())\n",
    "    print(f'{package_name} {installed_version} is installed, but {version} is required.')\n",
    "    # Optionally, upgrade or downgrade the package to the required version\n",
    "    call([sys.executable, '-m', 'pip', 'install', '--upgrade', package_version])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "from ENGR131_Util_2024 import cell_logger, StudentInfoForm, responses, upsert_to_json_file\n",
    "# Register the log function to be called before any cell is executed\n",
    "get_ipython().events.register('pre_run_cell', cell_logger)\n",
    "responses[\"assignment\"] = \"quiz_9_practice\"\n",
    "\n",
    "StudentInfoForm(**responses)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q1-ML-Resource\n",
    "points: 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Machine Learning Model Resource Estimator\n",
    "\n",
    "### Background\n",
    "\n",
    "In the field of machine learning and artificial intelligence, estimating the computational resources required for training a model is critical for efficiency and cost management. The total resource requirement includes both the static load (the base computational power and memory needed for the model architecture, datasets, and fixed algorithms) and the dynamic load (additional resources required for training iterations, data augmentation, and validation processes).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Implement a `ModelResourceEstimator` class to model the resource requirements for training a machine learning model.\n",
    "2. Include methods within this class to calculate the static load, dynamic load, and total resource requirement.\n",
    "3. Demonstrate the use of class initialization, basic math calculations, and functions calling other functions.\n",
    "\n",
    "### Class to Implement\n",
    "\n",
    "Implement a class `ModelResourceEstimator` with the following properties and methods:\n",
    "   - Methods:\n",
    "     - `__init__`: Initializes a new `ModelResourceEstimator` instance with the properties.\n",
    "         - Properties:\n",
    "           - `model_complexity` (a measure of model complexity, e.g., the number of parameters)\n",
    "           - `data_size` (in gigabytes, GB, representing the size of the dataset used for training)\n",
    "           - `iterations` (the number of training iterations)\n",
    "           - `base_resource_per_iteration` (in gigaflops per iteration, representing the base computational resource usage per training iteration)\n",
    "           - `memory_usage_static` (in gigabytes, GB, representing the static memory usage by the model and dataset)\n",
    "     - `static_load`: Calculates and returns the static resource load for training the model.\n",
    "       - The static load includes the base static memory usage.\n",
    "     - `dynamic_load`: Calculates and returns the dynamic resource load for the model based on the number of iterations and base resource per iteration.\n",
    "       - The dynamic load is calculated as the product of the number of iterations and the base resource usage per iteration.\n",
    "     - `total_resource_requirement`: Calculates and returns the total resource requirement by summing its static and dynamic loads.\n",
    "       - The total resource requirement is the sum of the static and dynamic loads.\n",
    "     - `print_resource_requirements`: Prints the static, dynamic, and total resource requirements in the following format:\n",
    "       - \"Static Load: {static_load} GB\\nDynamic Load: {dynamic_load} GFLOPs\\nTotal Resource Requirement: {total_resource_requirement} (GB for static load and GFLOPs for dynamic load)\" where the values are extracted from the object rounded to 2 decimal places.\n",
    "       - Note: you must use a **single print command**, \\n is used to create a new line in the print statement.\n",
    "\n",
    "Instantiate a `ModelResourceEstimator` object with the following properties:\n",
    "\n",
    "   - Model complexity: High (not directly quantified here, but influences resource estimation)\n",
    "   - Dataset size: 10 GB\n",
    "   - Number of training iterations: 1000\n",
    "   - Base computational resource per iteration: 5 gigaflops\n",
    "   - Static memory usage: 2 GB\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Class for a Machine Learning Model Resource Estimator goes Here\n",
    "# BEGIN SOLUTION\n",
    "class ModelResourceEstimator:\n",
    "    def __init__(self, model_complexity, data_size, iterations, base_resource_per_iteration, memory_usage_static):\n",
    "        \"\"\"\n",
    "        Initializes a new ModelResourceEstimator instance with the given properties.\n",
    "        \"\"\"\n",
    "        self.model_complexity = model_complexity\n",
    "        self.data_size = data_size\n",
    "        self.iterations = iterations\n",
    "        self.base_resource_per_iteration = base_resource_per_iteration\n",
    "        self.memory_usage_static = memory_usage_static\n",
    "\n",
    "    def static_load(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns the static resource load for training the model.\n",
    "        \"\"\"\n",
    "        return self.memory_usage_static\n",
    "\n",
    "    def dynamic_load(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns the dynamic resource load for the model.\n",
    "        \"\"\"\n",
    "        return self.iterations * self.base_resource_per_iteration\n",
    "\n",
    "    def total_resource_requirement(self):\n",
    "        \"\"\"\n",
    "        Calculates and returns the total resource requirement by summing its static and dynamic loads.\n",
    "        \"\"\"\n",
    "        return self.static_load() + self.dynamic_load()\n",
    "\n",
    "    def print_resource_requirements(self):\n",
    "        \"\"\"\n",
    "        Prints the static, dynamic, and total resource requirements.\n",
    "        \"\"\"\n",
    "        print(f\"Static Load: {self.static_load():.2f} GB\\nDynamic Load: {self.dynamic_load():.2f} GFLOPs\\nTotal Resource Requirement: {self.total_resource_requirement():.2f} (GB for static load and GFLOPs for dynamic load)\")\n",
    "# END SOLUTION\n",
    "\n",
    "# Instantiate a ModelResourceEstimator object with the specified properties\n",
    "# BEGIN SOLUTION\n",
    "model_resource_estimator = ModelResourceEstimator(model_complexity='High', data_size=10, iterations=1000, base_resource_per_iteration=5, memory_usage_static=2)\n",
    "# END SOLUTION\n",
    "\n",
    "# Print the static, dynamic, and total resource requirements\n",
    "# you can uncomment this line for testing\n",
    "# model_resource_estimator.print_resource_requirements()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 6\n",
    "success_message: All methods are implemented correctly.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if \"drexel_email\" not in responses:\n",
    "   raise ValueError(\"Please fill out the student info form and run the test again\")\n",
    "\n",
    "points_ = [6, 4, 5, 4, 4, 4, 4, 6, 5]\n",
    "for i, point in enumerate(points_):\n",
    "   drexel_jupyter_logger.variable_logger_csv(f\"0, {point}\", f\"q1_{i+1}\")\n",
    "\n",
    "\n",
    "# TODO: Add to student grader\n",
    "\n",
    "import json\n",
    "\n",
    "# Organize the data into a dictionary\n",
    "student_data = {\n",
    "   \"first_name\": responses[\"first_name\"],\n",
    "   \"last_name\": responses[\"last_name\"],\n",
    "   \"drexel_id\": responses[\"drexel_id\"],\n",
    "   \"drexel_email\": responses[\"drexel_email\"],\n",
    "}\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open('student_data.json', 'w') as json_file:\n",
    "   json.dump(student_data, json_file)\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_1\"\n",
    "\n",
    "max_score = 6\n",
    "score = 0\n",
    "for method in [\"__init__\",\"model_complexity\", \"data_size\", \"iterations\", \"base_resource_per_iteration\", \"memory_usage_static\"]:\n",
    "   if hasattr(model_resource_estimator, method):\n",
    "      score += 1\n",
    "      \n",
    "score = int(np.ceil(score))\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "               \"score\": score,\n",
    "               \"max_score\": max_score,\n",
    "               \"student_response\": dir(model_resource_estimator),\n",
    "               }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "   scorer.submit()\n",
    "\n",
    "# Assert that all required methods are implemented in the model_resource_estimator class\n",
    "assert hasattr(model_resource_estimator, \"__init__\"), \"__init__ method is not implemented\"\n",
    "assert hasattr(model_resource_estimator, \"model_complexity\"), \"model_complexity method is not implemented\"\n",
    "assert hasattr(model_resource_estimator, \"data_size\"), \"data_size method is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'iterations'), \"iterations method is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'base_resource_per_iteration'), \"base_resource_per_iteration method is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'memory_usage_static'), \"memory_usage_static method is not implemented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 4\n",
    "success_message: Init takes the correct number of parameters\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "\n",
    "if \"drexel_email\" not in responses:\n",
    "   raise ValueError(\"Please fill out the student info form and run the test again\")\n",
    "\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_2\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 2\n",
    "score = 0\n",
    "if len(ModelResourceEstimator.__init__.__code__.co_varnames) == 6:\n",
    "   score += 2\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "               \"score\": score,\n",
    "               \"max_score\": max_score,\n",
    "               \"student_response\": dir(model_resource_estimator),\n",
    "               }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "   scorer.submit()\n",
    "\n",
    "# Assert that all required methods are implemented in the ModelResourceEstimator class\n",
    "assert len(ModelResourceEstimator.__init__.__code__.co_varnames) == 6, \"__init__ method does not take the correct number of parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 5\n",
    "success_message: All properties are implemented correctly.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "import numpy as np\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_3\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 5\n",
    "score = 0\n",
    "for method in [\"model_complexity\", \"data_size\", \"iterations\", \"base_resource_per_iteration\", \"memory_usage_static\"]:\n",
    "    if hasattr(model_resource_estimator, method):\n",
    "        score += 1\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": dir(model_resource_estimator),\n",
    "            }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "    scorer.submit()\n",
    "\n",
    "# Assert that all required properties are implemented in the ModelResourceEstimator class\n",
    "assert hasattr(model_resource_estimator, 'model_complexity'), \"model_complexity attribute is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'data_size'), \"data_size attribute is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'iterations'), \"iterations attribute is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'base_resource_per_iteration'), \"base_resource_per_iteration attribute is not implemented\"\n",
    "assert hasattr(model_resource_estimator, 'memory_usage_static'), \"memory_usage_static attribute is not implemented\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 4\n",
    "success_message: static_load method is implemented correctly.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_4\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 4\n",
    "score = 0\n",
    "\n",
    "# Instantiate a ModelResourceEstimator object for testing\n",
    "m1 = ModelResourceEstimator(model_complexity='High', data_size=10, iterations=1000, base_resource_per_iteration=5, memory_usage_static=2)\n",
    "test = m1.static_load() == 2  # Expected static load based on the given static memory usage\n",
    "statement = \"static_load method is not implemented correctly\"\n",
    "value = m1.static_load()\n",
    "\n",
    "if test:\n",
    "    score += 4\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": value,\n",
    "            }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "    scorer.submit()\n",
    "\n",
    "# Assert that all required methods are implemented in the ModelResourceEstimator class\n",
    "assert test, statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 4\n",
    "success_message: static load implemented correctly\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_5\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 4\n",
    "score = 0\n",
    "\n",
    "m1 = ModelResourceEstimator(model_complexity='High', data_size=10, iterations=1000, base_resource_per_iteration=5, memory_usage_static=2)\n",
    "\n",
    "value = m1.static_load()\n",
    "test = m1.static_load() == 2\n",
    "statement = \"static load method is not implemented correctly\"\n",
    "\n",
    "if test:\n",
    "    score += 4\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": value,\n",
    "            }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "    scorer.submit()\n",
    "\n",
    "# Assert that all required methods are implemented in the ModelResourceEstimator class\n",
    "assert test, statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 4\n",
    "success_message: dynamic load implemented correctly\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "\n",
    "# Replace Beam with ModelResourceEstimator for the context of machine learning model resource estimation\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_6\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 4\n",
    "score = 0\n",
    "\n",
    "# Instantiate your ModelResourceEstimator with appropriate parameters\n",
    "estimator = ModelResourceEstimator(model_complexity='High', data_size=10, iterations=1000, base_resource_per_iteration=5, memory_usage_static=2)\n",
    "\n",
    "# We are checking the dynamic_load method now, instead of live_load\n",
    "value = estimator.dynamic_load()\n",
    "test = value == 5000  # Assuming 1000 iterations * 5 gigaflops per iteration\n",
    "statement = \"dynamic load method is not implemented correctly\"\n",
    "\n",
    "if test:\n",
    "    score += 4\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": value,\n",
    "            }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "    scorer.submit()\n",
    "\n",
    "# Assert that the dynamic load method is implemented correctly in the ModelResourceEstimator class\n",
    "assert test, statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 4\n",
    "success_message: Total resource requirement implemented correctly\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_7\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 4\n",
    "score = 0\n",
    "\n",
    "# Instantiate a ModelResourceEstimator object\n",
    "mre = ModelResourceEstimator(model_complexity='High', data_size=10, iterations=1000, base_resource_per_iteration=5, memory_usage_static=2)\n",
    "\n",
    "# Calculate the total resource requirement\n",
    "value = mre.total_resource_requirement()\n",
    "\n",
    "# Test the correctness of the total_resource_requirement method\n",
    "test = value == mre.static_load() + mre.dynamic_load()\n",
    "statement = \"Total resource requirement method is not implemented correctly\"\n",
    "\n",
    "if test:\n",
    "    score += 4\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "\n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": value,\n",
    "            }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "    scorer.submit()\n",
    "\n",
    "# Assert that the total_resource_requirement method is implemented correctly\n",
    "assert test, statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 6\n",
    "success_message: Input parameters are correctly defined.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from unittest.mock import patch\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_9\"  # Assuming a new question ID for clarity\n",
    "max_score = 3\n",
    "\n",
    "# Adjusted expected message to fit the ModelResourceEstimator output format\n",
    "expected_message = 'Static Load: 2.00 GB\\nDynamic Load: 5000.00 GFLOPs\\nTotal Resource Requirement: 5002.00 (GB for static load and GFLOPs for dynamic load)'\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "      # Updated instantiation to fit the ModelResourceEstimator class\n",
    "      mre = ModelResourceEstimator(model_complexity='High', data_size=10, iterations=1000, base_resource_per_iteration=5, memory_usage_static=2)\n",
    "      \n",
    "      # Call the method\n",
    "      mre.print_resource_requirements()\n",
    "      \n",
    "      mock_print.assert_called_once_with(expected_message)\n",
    "      \n",
    "if mock_print.call_args[0][0] == expected_message:\n",
    "      score = 6\n",
    "      drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "else:\n",
    "      score = 0\n",
    "   \n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": f\"{mock_print.call_args[0][0]}\",\n",
    "            }\n",
    "      \n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "      scorer.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "points: 5\n",
    "success_message: Input parameters are correctly defined.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import drexel_jupyter_logger\n",
    "from unittest.mock import patch\n",
    "from ENGR131_Util_2024 import submit_score\n",
    "import numpy as np \n",
    "\n",
    "scorer = submit_score()\n",
    "question_id = \"q1_9\"\n",
    "\n",
    "# Scoring logic as described\n",
    "max_score = 5\n",
    "score = 0\n",
    "# Adjust the attributes to fit the ModelResourceEstimator class\n",
    "attributes = [\"model_complexity\", \"data_size\", \"iterations\", \"base_resource_per_iteration\", \"memory_usage_static\"]\n",
    "values = ['High', 10, 1000, 5, 2]  # Assuming 'High' can be quantified or compared in some way\n",
    "\n",
    "for attribute, expected_value in zip(attributes, values):\n",
    "    actual_value = eval(f\"model_resource_estimator.{attribute}\")\n",
    "    # For the 'model_complexity', you might need a different validation logic\n",
    "    if attribute == \"model_complexity\":\n",
    "        if actual_value == expected_value:\n",
    "            score += 1\n",
    "    elif actual_value == expected_value:\n",
    "        score += 1\n",
    "\n",
    "drexel_jupyter_logger.variable_logger_csv(f\"{score}, {max_score}\", question_id)\n",
    "   \n",
    "response = {\"question_id\": question_id,\n",
    "            \"score\": score,\n",
    "            \"max_score\": max_score,\n",
    "            \"student_response\": f\"\"\"Model Complexity: {model_resource_estimator.model_complexity}, \n",
    "                                Data Size: {model_resource_estimator.data_size}, \n",
    "                                Iterations: {model_resource_estimator.iterations}, \n",
    "                                Base Resource per Iteration: {model_resource_estimator.base_resource_per_iteration}, \n",
    "                                Static Memory Usage: {model_resource_estimator.memory_usage_static}\n",
    "                                \"\"\",\n",
    "            }\n",
    "\n",
    "scorer.add_response(response)\n",
    "\n",
    "with patch('builtins.print') as mock_print:\n",
    "    scorer.submit()  \n",
    "\n",
    "# Update assertions for the new class attributes\n",
    "for attribute, expected_value in zip(attributes, values):\n",
    "    assert eval(f\"model_resource_estimator.{attribute}\") == expected_value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Your Assignment\n",
    "\n",
    "To submit your assignment please use the following link the assignment on GitHub classroom.\n",
    "   \n",
    "Use this [link](https://classroom.github.com/a/Ok5XgX3N) add link to navigate to the assignment on GitHub classroom.\n",
    "\n",
    "If you need further instructions on submitting your assignment please look at Lab 1. \n",
    "\n",
    "## Viewing your score\n",
    "\n",
    "Each `.ipynb` file you have uploaded will have a file with the name of your file + `Grade_Report.md`. You can view this file by clicking on the file name. This will show you the results of the autograder. \n",
    "\n",
    "We have both public and hidden tests. You will be able to see the score of both tests, but not the specific details of why the test passed or failed. \n",
    "\n",
    "```{note}\n",
    "In python and particularly jupyter notebooks it is common that during testing you run cells in a different order, or run cells and modify them. This can cause there to be local variables needed for your solution that would not be recreated on running your code again from scratch. Your assignment will be graded based on running your code from scratch. This means before you submit your assignment you should restart the kernel and run all cells. You can do this by clicking `Kernel` and selecting `Restart and Run All`. If you code does not run as expected after restarting the kernel and running all cells it means you have an error in your code. \n",
    "```\n",
    "\n",
    "## Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
