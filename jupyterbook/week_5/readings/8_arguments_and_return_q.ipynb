{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# You must make sure to run all cells in sequence using shift + enter or you might encounter errors\n",
    "from pykubegrader.initialize import initialize_assignment\n",
    "\n",
    "responses = initialize_assignment(\"8_arguments_and_return_q\", \"week_5\", \"readings\", assignment_points = 20.0, assignment_tag = 'week5-readings')\n",
    "\n",
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"8_arguments_and_return_q.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# ❓ Norman Doors - How, \"Not\" to Design Arguments, Parameters, and Return Statements \n",
    "\n",
    "![](./assets/figures/norman_door_2.png)\n",
    "\n",
    "A **Norman door** is a term for a poorly designed door that confuses users about whether to push or pull, often due to unclear visual or tactile cues. Coined by Don Norman in *The Design of Everyday Things*, it highlights the failure of the design to communicate its function intuitively, often requiring labels like \"PUSH\" or \"PULL\" to compensate. Good design avoids such confusion by aligning the door's appearance and affordances (e.g., handles or flat plates) with its intended operation, ensuring usability without explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this block of code by pressing Shift + Enter to display the question\n",
    "from questions._8_arguments_and_return_q import Question1\n",
    "Question1().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this block of code by pressing Shift + Enter to display the question\n",
    "from questions._8_arguments_and_return_q import Question2\n",
    "Question2().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Run this block of code by pressing Shift + Enter to display the question\n",
    "from questions._8_arguments_and_return_q import Question3\n",
    "Question3().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Create a Function to Evaluate Door Usability\n",
    "\n",
    "![](./assets/figures/norman-doors.jpg)\n",
    "\n",
    "Write a Python function named `evaluate_door` that:\n",
    "\n",
    "- Accepts two arguments: `handle_type` (a string) and `door_action` (a string).\n",
    "- Returns `\"Confusing\"` if:\n",
    "    - The handle type is \"pull\" but the action is \"push\".\n",
    "    - The handle type is \"push\" but the action is \"pull\".\n",
    "- Returns `\"Clear\"` if the handle type matches the action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "print(evaluate_door(\"pull\", \"push\")) # Should print Confusing\n",
    "print(evaluate_door(\"push\", \"push\")) # Should print Clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"Door-Assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Create a Function to Recommend Door Fixes\n",
    "\n",
    "![](./assets/figures/norman-3.avif)\n",
    "\n",
    "Write a Python function named `recommend_fix` that:\n",
    "- Accepts two arguments: `handle_type` (a string) and `door_action` (a string).\n",
    "- Returns a string recommendation:\n",
    "    - `\"Change the handle\"` if the handle type does not match the action.\n",
    "    - `\"No changes needed\"` if the handle type matches the action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "# Example usage\n",
    "print(recommend_fix(\"pull\", \"push\")) # Should print Change the handle\n",
    "print(recommend_fix(\"push\", \"push\")) # Should print No changes needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "grader.check(\"Door-Recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Submitting Assignment\n",
    "\n",
    "Please run the following block of code using `shift + enter` to submit your assignment, you should see your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from pykubegrader.submit.submit_assignment import submit_assignment\n",
    "\n",
    "submit_assignment(\"week5-readings\", \"8_arguments_and_return_q\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engr131_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "Door-Assessment": {
     "name": "Door-Assessment",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> max_question_points = str(6.0)\n>>> earned_points = 0\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> os.environ['TOTAL_POINTS_FREE_RESPONSE'] = str(11.0)\n>>> log_variable('total-points', f'Reading-Week-X, 8_arguments_and_return_q', 11.0)\n>>> question_id = 'Door-Assessment-1'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'pull'\n>>> door_action = 'push'\n>>> expected_output = 'Confusing'\n>>> actual_output = evaluate_door(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if (condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"):\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
         "failure_message": "Failed: The function does not correctly identify a pull handle with a push action as 'Confusing'.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly identifies a pull handle with a push action as 'Confusing'!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Assessment-2'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'push'\n>>> door_action = 'pull'\n>>> expected_output = 'Confusing'\n>>> actual_output = evaluate_door(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
         "failure_message": "Failed: The function does not correctly identify a push handle with a pull action as 'Confusing'.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly identifies a push handle with a pull action as 'Confusing'!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Assessment-3'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'pull'\n>>> door_action = 'pull'\n>>> expected_output = 'Clear'\n>>> actual_output = evaluate_door(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
         "failure_message": "Failed: The function does not correctly identify a pull handle with a pull action as 'Clear'.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly identifies a pull handle with a pull action as 'Clear'!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Assessment-4'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'push'\n>>> door_action = 'push'\n>>> expected_output = 'Clear'\n>>> actual_output = evaluate_door(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
         "failure_message": "Failed: The function does not correctly identify a push handle with a push action as 'Clear'.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly identifies a push handle with a push action as 'Clear'!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Assessment-5'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'rotate'\n>>> door_action = 'turn'\n>>> expected_output = 'Clear'\n>>> actual_output = evaluate_door(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
         "failure_message": "Failed: The function does not correctly handle invalid inputs.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly handles invalid inputs!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Assessment-6'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'rotate'\n>>> door_action = 'turn'\n>>> expected_output = 'Clear'\n>>> actual_output = evaluate_door(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n",
         "failure_message": "Failed: The function does not correctly handle invalid inputs.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly handles invalid inputs!"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Door-Recommendations": {
     "name": "Door-Recommendations",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> max_question_points = str(5.0)\n>>> earned_points = 0\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> os.environ['TOTAL_POINTS_FREE_RESPONSE'] = str(11.0)\n>>> log_variable('total-points', f'Reading-Week-X, 8_arguments_and_return_q', 11.0)\n>>> question_id = 'Door-Recommendations-1'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'pull'\n>>> door_action = 'push'\n>>> expected_output = 'Change the handle'\n>>> actual_output = recommend_fix(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if (condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"):\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(handle_type))\n>>> responses = update_responses(question_id, str(door_action))\n>>> responses = update_responses(question_id, str(expected_output))\n>>> responses = update_responses(question_id, str(actual_output))\n",
         "failure_message": "Failed: The function does not correctly recommend 'Change the handle' for mismatched input (pull, push).",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly recommends 'Change the handle' for mismatched input (pull, push)!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Recommendations-2'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'push'\n>>> door_action = 'pull'\n>>> expected_output = 'Change the handle'\n>>> actual_output = recommend_fix(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(handle_type))\n>>> responses = update_responses(question_id, str(door_action))\n>>> responses = update_responses(question_id, str(expected_output))\n>>> responses = update_responses(question_id, str(actual_output))\n",
         "failure_message": "Failed: The function does not correctly recommend 'Change the handle' for mismatched input (push, pull).",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly recommends 'Change the handle' for mismatched input (push, pull)!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Recommendations-3'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'pull'\n>>> door_action = 'pull'\n>>> expected_output = 'No changes needed'\n>>> actual_output = recommend_fix(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(handle_type))\n>>> responses = update_responses(question_id, str(door_action))\n>>> responses = update_responses(question_id, str(expected_output))\n>>> responses = update_responses(question_id, str(actual_output))\n",
         "failure_message": "Failed: The function does not correctly recommend 'No changes needed' for matched input (pull, pull).",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly recommends 'No changes needed' for matched input (pull, pull)!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Recommendations-4'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'push'\n>>> door_action = 'push'\n>>> expected_output = 'No changes needed'\n>>> actual_output = recommend_fix(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(handle_type))\n>>> responses = update_responses(question_id, str(door_action))\n>>> responses = update_responses(question_id, str(expected_output))\n>>> responses = update_responses(question_id, str(actual_output))\n",
         "failure_message": "Failed: The function does not correctly recommend 'No changes needed' for matched input (push, push).",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly recommends 'No changes needed' for matched input (push, push)!"
        },
        {
         "code": ">>> from pykubegrader.telemetry import ensure_responses, log_variable, score_question, submit_question, telemetry, update_responses\n>>> import os\n>>> question_id = 'Door-Recommendations-5'\n>>> max_score = 1.0\n>>> score = 0\n>>> handle_type = 'rotate'\n>>> door_action = 'turn'\n>>> expected_output = 'Change the handle'\n>>> actual_output = recommend_fix(handle_type, door_action)\n>>> condition = actual_output == expected_output\n>>> assert condition, f\"Expected '{expected_output}' for input ({handle_type}, {door_action}), but got '{actual_output}'.\"\n>>> if condition:\n...     score = 1.0\n>>> earned_points = float(os.environ.get('EARNED_POINTS', 0))\n>>> earned_points += score\n>>> log_variable('8_arguments_and_return_q', f'{score}, {max_score}', question_id)\n>>> os.environ['EARNED_POINTS'] = str(earned_points)\n>>> responses = update_responses(question_id, str(handle_type))\n>>> responses = update_responses(question_id, str(door_action))\n>>> responses = update_responses(question_id, str(expected_output))\n>>> responses = update_responses(question_id, str(actual_output))\n",
         "failure_message": "Failed: The function does not correctly handle invalid inputs.",
         "hidden": false,
         "locked": false,
         "points": 1,
         "success_message": "Success: The function correctly handles invalid inputs and recommends 'Change the handle'!"
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
